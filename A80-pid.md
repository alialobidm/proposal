A68: PID LB policy.
----
* Author(s): @s-matyukevich
* Approver: 
* Status: Draft
* Implemented in: PoC in Go
* Last updated: 2024-04-18
* Discussion at: 

## Abstract

This document proposes a design for a new load balancing policy `pid`. `pid` stands for [Proportional–integral–derivative controller](https://en.wikipedia.org/wiki/Proportional%E2%80%93integral%E2%80%93derivative_controller). This policy is built on top of [A58: weighted_round_robin LB policy (WRR)][A58] and also requires direct load reporting from backends to clients. Like `wrr` it also uses client-side weighted round robin load balancing. Unlike `wrr` it doesn't build weights deterministically, instead it uses feedback loop with `pid` controller to adjust the weights in such a way that load on all backends converge to the same value. The policy supports either per-call or periodic out-of-band load reporting per [gRFC A51][A51].

## Background

`wrr` uses the following formula to calculate the subchannel weights:

$$weight = \dfrac{qps}{utilization + \dfrac{eps}{qps} * error\\_utilization\\_penalty}$$

This works great in cases when backends have different average CPU cost per request and recieve identical number of connections. In such cases `wrr` helps to distribute requests between backends fairly, such that more powerfull backends recieve more requests and less powerfull backends recieve less requests. However, `wrr` doesn't help at all to correct the imbalance generated by usage of random subsetting, as described in [gRFC A68][A68] This is because the usage of random subsetting results in a state where some backends recieve more connections than others. The number of connections a server recieves doesn't affect its CPU cost per request metric, so more connected backends will be reciving more requests than less connected backends.

`pid` balancer takes a different aproach: instead of deterministically calculating weights based on some backend metric, it keeps adjusting weights at runtime and uses a feedback loop based on backend CPU metric to determine the direction and maginute for every weight update.

### Related Proposals:
* [gRFC A51][A51]
* [gRFC A52][A52]
* [gRFC A58][A58]
* [gRFC A68][A68]

## Proposal

Introduce a new LB policy `pid`. This policy implements client-side load balancing with direct load reports from backends. It uses feedback look with PID controller to tune the weights. The policy is otherwise largely a superset of the existing policy `weighted_round_robin`.

### LB Policy Config and Parameters

The `pid` LB policy config will be as follows.

```textproto
message LoadBalancingConfig {
  oneof policy {
    PIDLbConfig pid = 20 [json_name = "pid"];
  }
}

message PIDLbConfig {
  // The config for the WRR load balancer, as defined in [gRFC A58][A58]
  // PID balancer is an extenstion of WRR and all WRR settings apply to PID in an identical way. 
  WeightedRoundRobinLbConfig wrr_config = 1;

  // Only after eps/qps grows past this value the balancer starts taking into account ErrorUtilizationPenalty.
  // This is necessary to avoid oscilations in cases when server has very high and spiky error rate. 
  // Even in such cases we don't want to remove error_utilization_penalty completely as then we could
  // redirect all traffic to an instance that has low CPU and simply rejects all requests.
  // Default is 0.5.
  google.protobuf.FloatValue error_utilization_threshold = 2;

  // Controls how fast PID controller converges to mean value. Higher values speed up convergence
  // but can result in oscillations. Oscillations can happen if server load changes faster than
  // PID controller can react to the changes. This could happen if there are significant delays in
  // load report propagation (for example, due to use of OOB reporting with large load period) or if 
  // server load is very spiky. To deal with spiky server load server owners should use moving average 
  // on the server to smooth load function.
  // Default is 0.1.
  google.protobuf.FloatValue proportional_gain = 2;

  // Controls how smooth PID controller convergence is. Higher values makes it more smooth,
  // but can slow down convergence. 
  // Default is 1.
  google.protobuf.FloatValue derivative_gain = 4;

  // Max allowed weight. If PID controller attempts to set a higher weight it will be capped at this value.
  // This is necessary to prevent weights growing to infinity, which could happen if only a subset of clients
  // is using PID and we reached a point after which increasing weights no longer help to correct the imbalance.
  // The default is 10.
  google.protobuf.FloatValue max_weight = 5;

  // Min allowed weight. If PID controller attempts to set a lower weight it will be capped at this value.
  // This is necessary to prevent weights dropping to zero, which could happen if only a subset of clients
  // is using PID and we reached a point after which decreasing weights no longer help to correct the imbalance.
  // The default is 0.1
  google.protobuf.FloatValue min_weight = 6;
}
```

### PID controller

A PID controller is a control loop mechanism employing feedback. It continuously calculates an error value as the difference between a desired setpoint (`referenceSignal`) and a measured process variable (`actualSignal`) and applies a correction based on proportional, integral, and derivative terms (denoted P, I, and D respectively), hence the name.

In our implementation we won't be using integral part (It is usefull to speed up convergence when `referenceSignal` changes shraply. In our case we will be conveging the load on the subchannels to the mean value, which is mostly stable)

Here is a sample implementation in pseudo-code.

```
pidController class {
  proportionalGain float
  derivativeGain float

  controlError float

  update(referenceSignal float, actualSignal float, samplingInterval duration) float {
    previousError = this.controlError
    // save last controlError so we can use it to calculate derivative during next update
    this.controlError = referenceSignal - actualSignal
    controlErrorDerivative = (controlError - previousError) / samplingInterval.Seconds()
    controlSignal = this.proportionalGain*this.ontrolError +
      this.derivativeGain*this.controlErrorDerivative
  }
}
```

`update` method is expected to be called on a regular basis. `samplingInterval` is the duration since the last update. Return value is the control signal which, if applied to the system, should minimize control error. In the next section we'll discuss how control signal is converted to `wrr` weight.

`proportionalGain` and `derivativeGain` parameters are taken from the lb config. `proportionalGain` should be first scaled by the `WeightUpdatePeriod` value. This is because `controlErrorDerivative` is inversly proportional to the sampling interval, which in turn is close to `WeightUpdatePeriod` as we will be updating PID state once per `WeightUpdatePeriod`. If `WeightUpdatePeriod` is too small `controlErrorDerivative` becomes too large and dominates the resulting controll error. 

### Extending WRR balancer

`pid` balancer reuses 90% of `wrr` code. The proposal is to refactor `wrr` and add a few hooks to it so that other balancers (like `pid`) could reuse the code without copy-paste. We should keep those hooks internal (at least at first) to avoid locking ourselves into a new public API. This is mostly language specific, but the idea is to do the following:
* Add `callbacks` object to wrr balancer. This object contains a few callbacks that `wrr` will be calling during various stages of its lifetime.
* Add `callbackData` object, which will be used by callbacks to store any data that is reused between callbacks. The balancer will be passing it all callbacks and othwerwise treat it as an opaque blob of data.

`callbacks` object will be provided by the balancer builder. This object implements the following interface (written in pseudo-code)

```
wrrCallbacks interface {
  onSubchannelAdded(subchannelID int, data callbackData)

  onSubchannelRemoved(subchannelID int, data callbackData)

  // onLoadReport is called when a new load report is recieved for a given subchannel. 
  // This function returns the new weight for a subchannel. If returned value is -1
  // the subchannel should keep using the old value.
  // onLoadReport won't be called during blackout period.
  onLoadReport(subchannelId int, data callbackData, conf lbConfig, report loadReport) float

  // onEDFSchedulerUpdate is called after wrr balancer recreates the EDF scheduler.
  onEDFSchedulerUpdate(data callbackData)
}
```

`pid` balancer implements those callbacks as follows:

```
func onSubchannelAdded(subchannelID int) {
  // do nothing
}

func onSubchannelRemoved(subchannelID int) {
  // remove subchannelID from 2 maps 
  // that store the value of last utiliztion and 
  // last applied weight per subchannel
  delete(subchannelID, data.utilizationPerSubchannel)
  delete(subchannelID,data.lastAppliedWeightPerSubchannel)
}

func onLoadReport(subchannelId int, data callbackData, conf lbConfig, load loadReport, lastApplied time) float {
  utilization = load.ApplicationUtilization
	if utilization == 0 {
		utilization = load.CpuUtilization
	}
	if utilization == 0 || load.RpsFractional == 0 {
    // ignore empty load
		return -1
	}
  errorRate = load.Eps / load.RpsFractional
	useErrPenalty = errorRate > conf.ErrorUtilizationThreshold
	if useErrPenalty {
		utilization += errorRate * conf.ErrorUtilizationPenalty
	}

	// Make sure at least WeightUpdatePeriod has passed since we last updated PID state.
	// If we don't do that PID controller internal state may get corrupted in 2 ways:
	// * If 2 updates are very close to each other in time, samplingInterval ~= 0 and signal ~= infinity.
	// * If multiple updates happened during a single WeightUpdatePeriod, the actual weights are not applied,
	// but PID controller keep growing the weight and it may easily pass the balancing point.
	if time.Since(lastApplied) < conf.WeightUpdatePeriod {
		return -1
	}

  // use value calculated in the onEDFSchedulerUpdate method
	meanUtilization = data.meanUtilization

  // call PID controlelr to get the value of the control signal.
	controlSignal = data.pidController.update({
		referenceSignal:  meanUtilization,
		actualSignal:     utilization,
		samplingInterval: time.Since(lastApplied),
	})

	// Normalize the signal.
	// If meanUtilization ~= 0 the signal will be ~= 0 as well, and convergence will becoma painfully slow.
	// If, meanUtilization >> 1 the signal may become very high, which could lead to oscillations.
	if meanUtilization > 0 {
		controlSignal *= 1 / meanUtilization
	}

  lastAppliedWeight = data.lastAppliedWeightPerSubchannel[subchannelID]

  // Use controlSignal to adjust the weight.
  // First caclulate multiplier that will be used to determine how much weight should be changed.
  // The higher is the absolute value of the controlSignal the more we need to adjust the weight.
	if controlSignal >= 0 {
    // in this case mult should belong to [1,inf) interval, so we will be increasing the weight.
		mult = 1.0 + controlSignal
	} else {
    // in this case mult should belong to (0, 1) interval, so we will be decreasing the weight.
		mult = -1.0 / (controlSignal - 1.0)
	}
	weight = lastAppliedWeight * mult
  // clamp weight at min/max values to avoid growing to infinity or zero.
	if weight > conf.MaxWeight {
		weight = conf.MaxWeight
	}
	if weight < conf.MinWeight {
		weight = conf.MinWeight
	}

  // save resulting utilization and weight.
	data.utilizationPerSubchannel[subchannelId] = utilization
  data.lastAppliedWeightPerSubchannel[subchannelID] = weight

  return weight
}

func onEDFSchedulerUpdate(data callbackData) {
  // simply calculate mean utilization for all subchannels
  sum = 0
  foreach key, value in data.utilizationPerSubchannel {
    sum += value
  }
  data.meanUtilization = sum / data.utilizationPerSubchannel.length()
}
```

### Deling with oscilations

The main problem with using `pid` balancer is the probability of oscilations. This probability depends on the following factors

* How fast load reports are propagated to clients. The larger is the delay the higher is the chances that `pid` balancer will keep adjusting weights in the wrong direction, which could lead to oscilations. There are 2 cases we should consider:
  * `Direct load reporting`. In this case propagation delay depends on the request frequency and `WeightUpdatePeriod` setting. In practise this results in very fast propagation with default `WeightUpdatePeriod` value (1s) and this is the prefered option when using `pid`
  * `OOB load reporting`. In this case users can control the delay by using `OobReportingPeriod` setting. The delay in this case is usually much larger, still it is possible to acieve perfect convergence with OOB reporting on workloads with stable load.
* `ProportionalGain` value. If it is too high `pid` balancer will be making big adjustments to the weights and may pass the balancing point. Default value (0.1) result in relatively fast convergence (usually faster than 30 sec) on non spiky workloads.
* How stable is server load. `pid` balancer don't work very well with servers that have spiky load. The main reason for this is that mean utilization is not stable, which constantly disturb the convergence direction for all subchannels. This is the only property users can't directly control on the clinet side. The proposal is to add an "average window" mechanism to deal with it on the server. This will be discussed in the next section.
* How large is the number of subchannels. The larger it is, the more stable mean utilization is, which result in faster convergence and no oscilatioins. This is directly related to the usage of random subsetting discussed in [gRFC A68][A68]. If someone chooses too small subset size `pid` may have hard time converging the load across backends both because mean utilization is unstable and because a lot of clients may get connection only to overloaded or only to underloaded server - such clients won't contribute much to achieving overal convergence. Still we were able to get ok convergence on a spiky workload with ridiculasly small subset size of 4 with 3 minutes moving average window size for load reporting. Proposed default subset size of 20 usually results in good convergence on any workload.

### Moving awerage window for load reporting

As mentioned in the prvious section, we need a mechanizm to make utilization more smooth in the server load reports, otherwise `pid` balancer might not be able to acieve convergence on spiky workloads. The proposed solution is to use moving average  window when reporting results for a particular backend metrics. We should extend `MetricRecorder` component, which is desribed in [gRFC A51][A51] and add `MovingAverageWindowSize` parameter to it. Intead of storing a single value per metric, `MetricRecorder` now will store `MovingAverageWindowSize` last reported values. Whenever `recordMetricXXX` method is called `MetricRecorder` will add the new value to the circular buffer and remove the oldest value from the same buffer. This is ilustrated in the follwing pseudo-code example

```
func recordMetricXXX(value float) {
  // make sure the updates are atomic, otherwise we risk getting corrupted cicular buffer
  lock.Lock()
  // this automatically removes the last added value if cicular buffer is full
  circularBufferForMerricXXX.add(value)

  sum = 0
  foreach val in circularBufferForMerricXXX {
    sum += val
  }
  metricXXXvalue = sum/circularBufferForMerricXXX.size()
  lock.Unlock()
}
```

Setting `MovingAverageWindowSize` is identical to using tu current behaviour, and should be the default.



[A51]: A51-custom-backend-metrics.md
[A58]: A58-client-side-weighted-round-robin-lb-policy.md
[A68]: https://github.com/grpc/proposal/pull/423